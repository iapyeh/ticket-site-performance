## curl 的參數
## curl-format.txt
curlformat='%{speed_download},%{size_download},%{time_namelookup},%{time_connect},%{time_appconnect},%{time_pretransfer},%{time_redirect},%{time_starttransfer},%{time_total}'
## curl-header.txt
curlheader='''User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36
Cache-Control: max-age=0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8" -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36
HTTPS: 1
DNT: 1
Referer: https://www.google.com/
Accept-Language: en-US,en;q=0.8,en-GB;q=0.6,es;q=0.4'''

import os,json,time

## 取得tixcraft.com的ip
def getIPAddressFromJson(file):
    if os.path.exists(file):
      ## 在自己的環境中使用 domaindigger.spy抓的
      with open(file) as fd:
          cache = json.load(fd)
    else:
        ## 作者的環境抓到的
        cache = {"52.10.156.99": 1, "52.36.151.225": 1, "54.213.161.131": 1, "44.241.72.135": 1, "54.148.205.54": 1, "35.161.24.81": 1, "52.33.179.148": 1, "44.241.141.211": 1, "54.69.224.251": 1, "52.42.204.158": 1, "52.40.230.13": 1, "35.83.52.40": 1, "44.238.229.107": 1, "54.68.31.28": 1, "34.218.7.41": 1, "44.237.21.221": 1, "100.21.142.81": 1, "52.35.219.129": 1, "44.239.183.111": 1, "44.240.141.116": 1, "34.211.213.203": 1, "35.164.66.242": 1, "35.80.201.173": 1, "52.11.217.132": 1, "54.218.157.191": 1, "52.42.108.187": 1, "52.33.10.19": 1, "52.34.154.36": 1, "54.190.31.29": 1, "54.186.41.107": 1}
    return list(cache.keys())

class CURLDownload(object):
    sno = 0
    def __init__(self,ip,domain,outputFile=None,formatFile=None,headerFile=None):
        CURLDownload.sno += 1
        self.ip = ip
        self.domain = domain

        myfolder = os.path.dirname(os.path.abspath(__file__))
        if headerFile:
            assert os.path.exists(headerFile), f'{os.path.abspath(headerFile)} not found'
            self.headerFileGenerated = None
        else:
            headerFile = os.path.join(myfolder,f'curl-header-{ip}-{CURLDownload.sno}.txt')
            if not os.path.exists(headerFile):
                ## create header file
                with open(headerFile,'w') as fd:
                    fd.write(curlheader)
            self.headerFileGenerated = headerFile
        if formatFile:
            assert os.path.exists(formatFile), f'{os.path.abspath(formatFile)} not found'
            self.formatFileGenerated = None
        else:
            formatFile = os.path.join(myfolder,f'curl-format-{ip}-{CURLDownload.sno}.txt')
            if not os.path.exists(formatFile):
                ## create format file
                with open(formatFile,'w') as fd:
                    fd.write(curlformat)
            self.formatFileGenerated = formatFile

        self.headerFile = headerFile
        self.formatFile = formatFile
        assert os.path.exists(self.headerFile),f'{self.headerFile} not found'
        assert os.path.exists(self.formatFile),f'{self.formatFile} not found'
        self.fields = 'speed_download,size_download,time_namelookup,time_connect,time_appconnect,time_pretransfer,time_redirect,time_starttransfer,time_total'.split(',')
        #self.cmd = 'curl -k --http2 --compressed'
        self.cmd = 'curl -k --compressed'
        if outputFile:
            self.cmd +=' -o ' + outputFile
        else:
            self.cmd +=' -o /dev/null'
        self.cmd += f' -H "Host: {domain}:443"'
        if headerFile:
            self.cmd += f' -H "@{headerFile}"'
        if formatFile:
            self.cmd += f' -w "@{formatFile}"'
        self.cmd +=' -s https://' + ip

    def close(self):
        if self.formatFileGenerated  and os.path.exists(self.formatFileGenerated):
            os.unlink(self.formatFileGenerated)
        if self.headerFileGenerated and os.path.exists(self.headerFileGenerated):
            os.unlink(self.headerFileGenerated)

    def __del__(self):
        self.close()

    def downloadOne(self):
        #cmd = f'{self.cmd}'
        assert os.path.exists(self.headerFile),f'{self.headerFile} not found'
        assert os.path.exists(self.formatFile),f'{self.formatFile} not found'
        stdout,stderr,exitcode = $f'{self.cmd}'
        if exitcode == 0:
            data = [float(x) for x in stdout.split(',')]
            result = dict(zip(self.fields,data))
            return True, result
        else:
            return False, stderr

    def downloadMany(self,howmany,sleep=1):
        results = []
        for i in range(howmany):
            ok,result = self.downloadOne()
            if ok:
                results.append(result)
            else:
                print(f'{self.ip}, error:{result}')
            if sleep: time.sleep(sleep)

        avg = {}
        for field in self.fields:
            avg[field] = {'count':0,'average':0,'total':0}
            for result in results:
                avg[field]['count'] += 1
                avg[field]['total'] += result[field]
            avg[field]['average'] += round(avg[field]['total'] / avg[field]['count'],2)
        return avg, results

## 拓元售票網
domain = 'tixcraft.com'
## 取得所有連線的IP
ipAddresses = getIPAddressFromJson(os.path.join(domain+'.json'))
## 先找出最快與最慢的IP
fastIp = None
slowIp = None
for ip in ipAddresses[:]:
    cd = CURLDownload(ip,domain)    
    ## 下載3次取平均
    avg, results = cd.downloadMany(3)
    print(f"{ip}:{avg['speed_download']}")
    if fastIp is None:
        fastIp = {'ip':ip,'speed_download':avg['speed_download']['average']}
    elif avg['speed_download']['average'] > fastIp['speed_download']:
        fastIp = {'ip':ip,'speed_download':avg['speed_download']['average']}
    if slowIp  is None:
        slowIp = {'ip':ip,'speed_download':avg['speed_download']['average']}
    elif avg['speed_download']['average'] < slowIp['speed_download']:
        slowIp = {'ip':ip,'speed_download':avg['speed_download']['average']}
    cd.close()

## 觀察最快與最慢的IP有多少差距
print('-' * 20)
print('fastIp',fastIp)
print('slowIp',slowIp)
print('-' * 20)
## 再測試一次最快與最慢的IP有多少差距
for ip in (fastIp['ip'],slowIp['ip']):
    cd = CURLDownload(ip,domain)
    speed_download_total = 0
    ## 再多測3次，所以一共是下載9次取平均
    times = 3
    for i in range(times):
        avg, results = cd.downloadMany(3)
        print(f"{ip}:{str(i+1).zfill(3)}:{avg['speed_download']}")
        speed_download_total += avg['speed_download']['average']
    speed_download_average = round(speed_download_total / times,2)
    print(f"{ip}:Average Downlaod Speed:{speed_download_average} bytes/second")
    cd.close()
## （在我的環境下）我測試的結論是「差異不大」，拓元售票網的IP雖然很多，但可能很集中或者距離台灣的網路都一樣遙遠，
## 所以連到哪一個IP的差異不大。會在不同的ISP得到不一樣的速度主要原因是ISP的差異。
